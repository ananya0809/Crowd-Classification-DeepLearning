{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Project\n",
    "### Crowd Classification using Deep Learning, Computer Vision and Decision Tree\n",
    "### Guide: Dr. Sunil Kumar\n",
    "### Students: Ananya Agrawal (199303010) & Hardik Srivastava (199303069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training a CNN Model to create crowd heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Tensorflow inference to generate heatmap of crowd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"B\"\n",
    "train_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/train/'\n",
    "train_den_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/train_den/'\n",
    "val_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/val/'\n",
    "val_den_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/val_den/'\n",
    "test_path = './data/original/shanghaitech/part_' + dataset + '_final/test_data/images/'\n",
    "test_den_path = './data/original/shanghaitech/part_' + dataset + '_final/test_data/ground_truth_csv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 3249 Loaded\n",
      "100 / 3249 Loaded\n",
      "200 / 3249 Loaded\n",
      "300 / 3249 Loaded\n",
      "400 / 3249 Loaded\n",
      "500 / 3249 Loaded\n",
      "600 / 3249 Loaded\n",
      "700 / 3249 Loaded\n",
      "800 / 3249 Loaded\n",
      "900 / 3249 Loaded\n",
      "1000 / 3249 Loaded\n",
      "1100 / 3249 Loaded\n",
      "1200 / 3249 Loaded\n",
      "1300 / 3249 Loaded\n",
      "1400 / 3249 Loaded\n",
      "1500 / 3249 Loaded\n",
      "1600 / 3249 Loaded\n",
      "1700 / 3249 Loaded\n",
      "1800 / 3249 Loaded\n",
      "1900 / 3249 Loaded\n",
      "2000 / 3249 Loaded\n",
      "2100 / 3249 Loaded\n",
      "2200 / 3249 Loaded\n",
      "2300 / 3249 Loaded\n",
      "2400 / 3249 Loaded\n",
      "2500 / 3249 Loaded\n",
      "2600 / 3249 Loaded\n",
      "2700 / 3249 Loaded\n",
      "2800 / 3249 Loaded\n",
      "2900 / 3249 Loaded\n",
      "3000 / 3249 Loaded\n",
      "3100 / 3249 Loaded\n",
      "3200 / 3249 Loaded\n",
      "Training Data Loaded\n"
     ]
    }
   ],
   "source": [
    "training_images = os.listdir(train_path)\n",
    "num_training_images = len(training_images)\n",
    "\n",
    "train_data = []\n",
    "for i in range(num_training_images):\n",
    "    if i % 100 == 0:\n",
    "        print(i, '/', num_training_images, \"Loaded\")\n",
    "    name = training_images[i]\n",
    "    img = cv2.imread(train_path + name, 0)\n",
    "    img = np.array(img)\n",
    "    img = (img - 127.5) / 128\n",
    "    den = np.loadtxt(open(train_den_path + name[:-4] + '.csv'), delimiter = \",\")\n",
    "    den_quarter = np.zeros((int(den.shape[0] / 4), int(den.shape[1] / 4)))\n",
    "    for i in range(len(den_quarter)):\n",
    "        for j in range(len(den_quarter[0])):\n",
    "            for p in range(4):\n",
    "                for q in range(4):\n",
    "                    den_quarter[i][j] += den[i * 4 + p][j * 4 + q]\n",
    "    train_data.append([img, den_quarter])\n",
    "print('Training Data Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 316\n",
      "50 / 316\n",
      "100 / 316\n",
      "150 / 316\n",
      "200 / 316\n",
      "250 / 316\n",
      "300 / 316\n",
      "Test data Loaded\n"
     ]
    }
   ],
   "source": [
    "test_images = os.listdir(test_path)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "test_data = []\n",
    "for i in range(num_test_images):\n",
    "    if i % 50 == 0:\n",
    "        print(i, '/', num_test_images)\n",
    "    name = 'IMG_' + str(i + 1) + '.jpg'\n",
    "    img = cv2.imread(test_path + name, 0)\n",
    "    img = np.array(img)\n",
    "    img = (img - 127.5) / 128\n",
    "    den = np.loadtxt(open(test_den_path + name[:-4] + '.csv'), delimiter = \",\")\n",
    "    den_quarter = np.zeros((int(den.shape[0] / 4), int(den.shape[1] / 4)))\n",
    "    for i in range(len(den_quarter)):\n",
    "        for j in range(len(den_quarter[0])):\n",
    "            for p in range(4):\n",
    "                for q in range(4):\n",
    "                    den_quarter[i][j] += den[i * 4 + p][j * 4 + q]\n",
    "    test_data.append([img, den_quarter])\n",
    "        \n",
    "print('Test data Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling data around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate X, Y training and testing data lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for d in train_data:\n",
    "    X_train.append(np.reshape(d[0], (d[0].shape[0], d[0].shape[1], 1)))\n",
    "    y_train.append(np.reshape(d[1], (d[1].shape[0], d[1].shape[1], 1)))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for d in test_data:\n",
    "    X_test.append(np.reshape(d[0], (d[0].shape[0], d[0].shape[1], 1)))\n",
    "    y_test.append(np.reshape(d[1], (d[1].shape[0], d[1].shape[1], 1)))\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup helper methods to calculate Mean Absolute Error, Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(y_real, y_pred):\n",
    "    return abs(K.sum(y_real) - K.sum(y_pred))\n",
    "\n",
    "def calc_mse(y_real, y_pred):\n",
    "    return ((K.sum(y_real) - K.sum(y_pred)) * (K.sum(y_real) - K.sum(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 06:11:53.605409: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-20 06:11:53.605599: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-20 06:11:53.606000: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bigfloppa): /proc/driver/nvidia/version does not exist\n",
      "2022-05-20 06:11:53.607157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/hardik/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (None, None, 1))\n",
    "conv_m = Conv2D(20, (7, 7), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_m = MaxPooling2D(pool_size = (2, 2))(conv_m)\n",
    "conv_m = (conv_m)\n",
    "conv_m = Conv2D(40, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "conv_m = MaxPooling2D(pool_size = (2, 2))(conv_m)\n",
    "conv_m = Conv2D(20, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "conv_m = Conv2D(10, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "#conv_m = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_m)\n",
    "\n",
    "conv_s = Conv2D(24, (5, 5), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_s = MaxPooling2D(pool_size = (2, 2))(conv_s)\n",
    "conv_s = (conv_s)\n",
    "conv_s = Conv2D(48, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "conv_s = MaxPooling2D(pool_size = (2, 2))(conv_s)\n",
    "conv_s = Conv2D(24, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "conv_s = Conv2D(12, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "#conv_s = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_s)\n",
    "\n",
    "conv_l = Conv2D(16, (9, 9), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_l = MaxPooling2D(pool_size = (2, 2))(conv_l)\n",
    "conv_l = (conv_l)\n",
    "conv_l = Conv2D(32, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "conv_l = MaxPooling2D(pool_size = (2, 2))(conv_l)\n",
    "conv_l = Conv2D(16, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "conv_l = Conv2D(8, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "#conv_l = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_l)\n",
    "\n",
    "conv_merge = Concatenate(axis = 3)([conv_m, conv_s, conv_l])\n",
    "result = Conv2D(1, (1, 1), padding = 'same')(conv_merge)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = result)\n",
    "\n",
    "adam = Adam(lr = 1e-4)\n",
    "model.compile(loss = 'mse', optimizer = adam, metrics = [calc_mae, calc_mse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing model (while occasionally saving model and weights to disk for checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 556/2599 [=====>........................] - ETA: 1:04 - loss: 2.3923e-04 - calc_mae: 7.9498 - calc_mse: 194.0198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hardik/Projects/Minor-Project/new/final.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardik/Projects/Minor-Project/new/final.ipynb#ch0000054?line=3'>4</a>\u001b[0m best_mse_mae \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardik/Projects/Minor-Project/new/final.ipynb#ch0000054?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hardik/Projects/Minor-Project/new/final.ipynb#ch0000054?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hardik/Projects/Minor-Project/new/final.ipynb#ch0000054?line=8'>9</a>\u001b[0m     score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test, batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hardik/Projects/Minor-Project/new/final.ipynb#ch0000054?line=9'>10</a>\u001b[0m     score[\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(score[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1401'>1402</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1402'>1403</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1403'>1404</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1404'>1405</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1405'>1406</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1406'>1407</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1407'>1408</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1408'>1409</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1409'>1410</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1410'>1411</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2449'>2450</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2450'>2451</a>\u001b[0m   (graph_function,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2451'>2452</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2452'>2453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=2453'>2454</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1859'>1860</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1860'>1861</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1861'>1862</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1862'>1863</a>\u001b[0m     args,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1863'>1864</a>\u001b[0m     possible_gradient_type,\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1864'>1865</a>\u001b[0m     executing_eagerly)\n\u001b[1;32m   <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=1865'>1866</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=494'>495</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=495'>496</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=508'>509</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py?line=509'>510</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     <a href='file:///home/hardik/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_mae = 10000\n",
    "best_mae_mse = 10000\n",
    "best_mse = 10000\n",
    "best_mse_mae = 10000\n",
    "\n",
    "for i in range(200):\n",
    "    model.fit(X_train, y_train, epochs = 3, batch_size = 1, validation_split = 0.2)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "    score[2] = math.sqrt(score[2])\n",
    "    print(score)\n",
    "    if score[1] < best_mae:\n",
    "        best_mae = score[1]\n",
    "        best_mae_mse = score[2]\n",
    "        json_string = model.to_json()\n",
    "        open('model.json', 'w').write(json_string)\n",
    "        model.save_weights('weights.h5')\n",
    "    if score[2] < best_mse:\n",
    "        best_mse = score[2]\n",
    "        best_mse_mae = score[1]\n",
    "\n",
    "    print('Best mae: ', best_mae, '(', best_mae_mse, ')')\n",
    "    print('Best mse: ', '(', best_mse_mae, ')', best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 03:57:36.530766: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-05-20 03:57:36.538928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-20 03:57:36.538945: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "from pyheatmap.heatmap import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model and Weights from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 03:57:41.206210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-20 03:57:41.206316: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-20 03:57:41.206337: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bigfloppa): /proc/driver/nvidia/version does not exist\n",
      "2022-05-20 03:57:41.206566: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "model.load_weights(\"weights.h5\")\n",
    "\n",
    "print(\"Loaded model from disk successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up paths of image and other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = \"/home/hardik/Projects/Minor-Project/new/IMG_233.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing input image to correspond to input layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(cv2.imread(input_image, 0))\n",
    "img = (img - 127.5) / 128\n",
    "inputs = np.reshape(img, [1, 768, 1024, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 296ms/step\n"
     ]
    }
   ],
   "source": [
    "outputs = model.predict(inputs)\n",
    "predicted_count = np.sum(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Count: 27.358072\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Count:\", predicted_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating density numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_den = np.reshape(outputs, (outputs.shape[1], outputs.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap Generated to output.png\n"
     ]
    }
   ],
   "source": [
    "den_resized = np.zeros((y_p_den.shape[0] * 4, y_p_den.shape[1] * 4))\n",
    "for i in range(den_resized.shape[0]):\n",
    "    for j in range(den_resized.shape[1]):\n",
    "        den_resized[i][j] = y_p_den[int(i / 4)][int(j / 4)] / 16\n",
    "den = den_resized\n",
    "count = np.sum(den)\n",
    "den = den * 10 / np.max(den)\n",
    "\n",
    "crowd_img = cv2.imread(input_image, 1)\n",
    "\n",
    "data = []\n",
    "pts = []\n",
    "\n",
    "for j in range(len(den)):\n",
    "    for i in range(len(den[0])):\n",
    "        for k in range(int(den[j][i])):\n",
    "            data.append([i + 1, j + 1])\n",
    "            pts.append((i + 1, j + 1))\n",
    "\n",
    "hm = HeatMap(data, base=input_image)\n",
    "hm.heatmap(save_as = 'output.png')\n",
    "print(\"Heatmap Generated to output.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load data and create Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification (Target Label Generation) using Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMG_NUM</th>\n",
       "      <th>SOURCE_DATASET</th>\n",
       "      <th>AB</th>\n",
       "      <th>AC</th>\n",
       "      <th>AD</th>\n",
       "      <th>AE</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>BE</th>\n",
       "      <th>CD</th>\n",
       "      <th>CE</th>\n",
       "      <th>DE</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>226.214500</td>\n",
       "      <td>6.403124</td>\n",
       "      <td>174.287119</td>\n",
       "      <td>12.041595</td>\n",
       "      <td>86.815897</td>\n",
       "      <td>9.219544</td>\n",
       "      <td>6.324555</td>\n",
       "      <td>9.486833</td>\n",
       "      <td>7.071068</td>\n",
       "      <td>8.485281</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>A</td>\n",
       "      <td>339.676317</td>\n",
       "      <td>62.801274</td>\n",
       "      <td>152.947703</td>\n",
       "      <td>70.710678</td>\n",
       "      <td>42.059482</td>\n",
       "      <td>57.454330</td>\n",
       "      <td>354.407957</td>\n",
       "      <td>84.504438</td>\n",
       "      <td>49.091751</td>\n",
       "      <td>385.149322</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>341</td>\n",
       "      <td>A</td>\n",
       "      <td>71.168813</td>\n",
       "      <td>322.076078</td>\n",
       "      <td>307.483333</td>\n",
       "      <td>52.239832</td>\n",
       "      <td>234.326695</td>\n",
       "      <td>557.288076</td>\n",
       "      <td>306.778422</td>\n",
       "      <td>542.860940</td>\n",
       "      <td>486.864458</td>\n",
       "      <td>43.680659</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>378</td>\n",
       "      <td>A</td>\n",
       "      <td>302.828334</td>\n",
       "      <td>21.633308</td>\n",
       "      <td>243.895469</td>\n",
       "      <td>37.483330</td>\n",
       "      <td>550.905618</td>\n",
       "      <td>728.715308</td>\n",
       "      <td>21.095023</td>\n",
       "      <td>294.183616</td>\n",
       "      <td>283.444880</td>\n",
       "      <td>493.852205</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>26.172505</td>\n",
       "      <td>41.109610</td>\n",
       "      <td>287.141080</td>\n",
       "      <td>368.989160</td>\n",
       "      <td>29.732137</td>\n",
       "      <td>41.629317</td>\n",
       "      <td>31.622777</td>\n",
       "      <td>40.607881</td>\n",
       "      <td>215.520301</td>\n",
       "      <td>25.495098</td>\n",
       "      <td>dense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IMG_NUM SOURCE_DATASET          AB          AC          AD          AE  \\\n",
       "0       28              A  226.214500    6.403124  174.287119   12.041595   \n",
       "1       42              A  339.676317   62.801274  152.947703   70.710678   \n",
       "2      341              A   71.168813  322.076078  307.483333   52.239832   \n",
       "3      378              A  302.828334   21.633308  243.895469   37.483330   \n",
       "4       14              A   26.172505   41.109610  287.141080  368.989160   \n",
       "\n",
       "           BC          BD          BE          CD          CE          DE  \\\n",
       "0   86.815897    9.219544    6.324555    9.486833    7.071068    8.485281   \n",
       "1   42.059482   57.454330  354.407957   84.504438   49.091751  385.149322   \n",
       "2  234.326695  557.288076  306.778422  542.860940  486.864458   43.680659   \n",
       "3  550.905618  728.715308   21.095023  294.183616  283.444880  493.852205   \n",
       "4   29.732137   41.629317   31.622777   40.607881  215.520301   25.495098   \n",
       "\n",
       "  TARGET  \n",
       "0  dense  \n",
       "1  dense  \n",
       "2  dense  \n",
       "3  dense  \n",
       "4  dense  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_data = pd.read_csv(\"exported.csv\", index_col=0)\n",
    "cluster_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing X and Y slices of data, where X is our source data and Y contains the Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cluster_data.values[:, 2:-1]\n",
    "Y = cluster_data.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up functions to create decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex.\n",
    "def train_using_gini(X_train, y_train):\n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=None, min_samples_leaf=5)\n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "\n",
    "# Function to make predictions\n",
    "def prediction(X_test, clf_object):  \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = clf_object.predict(X_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = train_using_gini(X, Y)\n",
    "\n",
    "\n",
    "# y_pred_gini = prediction(X_test, clf_gini)\n",
    "# cal_accuracy(y_test, y_pred_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mat4py import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creates clusters of pts list\n",
    "def make_clusters(pts):\n",
    "    est = KMeans(5)\n",
    "    est.fit(pts)\n",
    "    y_kmeans = est.predict(pts)\n",
    "    # cluster list\n",
    "    cluster_list = [[], [], [], [], []]\n",
    "    for index in range(len(pts)):\n",
    "        cluster_list[y_kmeans[index]].append(pts[index])\n",
    "    return cluster_list\n",
    "\n",
    "# returns dist between 2 points\n",
    "def pt_dist(p1, p2):\n",
    "    xx = p1[0] - p2[0]\n",
    "    yy = p1[1] - p2[1]\n",
    "    return math.sqrt(xx*xx + yy*yy)\n",
    "\n",
    "# returns min dist between cluster 1 and cluster 2\n",
    "def min_dist(c1, c2):\n",
    "    min = 9999999\n",
    "    for p1 in c1:\n",
    "        for p2 in c2:\n",
    "            d = pt_dist(p1, p2)\n",
    "            if min > d:\n",
    "                min = d\n",
    "    return min\n",
    "\n",
    "all_clusters = make_clusters(pts)\n",
    "\n",
    "ab = min_dist(all_clusters[0], all_clusters[1])\n",
    "ac = min_dist(all_clusters[0], all_clusters[2])\n",
    "ad = min_dist(all_clusters[0], all_clusters[3])\n",
    "ae = min_dist(all_clusters[0], all_clusters[4])\n",
    "bc = min_dist(all_clusters[1], all_clusters[2])\n",
    "bd = min_dist(all_clusters[1], all_clusters[3])\n",
    "be = min_dist(all_clusters[1], all_clusters[4])\n",
    "cd = min_dist(all_clusters[2], all_clusters[3])\n",
    "ce = min_dist(all_clusters[2], all_clusters[4])\n",
    "de = min_dist(all_clusters[3], all_clusters[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[563.4696087634186, 81.00617260431454, 301.24076749337894, 1.0, 282.71894170713074, 1.0, 373.5906851087163, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "check_entry = [ab, ac, ad, ae, bc, bd, be, cd, ce, ce]\n",
    "print(check_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "['dense']\n"
     ]
    }
   ],
   "source": [
    "y_pred_gini = prediction([check_entry], clf_gini)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
