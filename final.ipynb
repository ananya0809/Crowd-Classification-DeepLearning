{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minor Project\n",
    "### Crowd Classification using Deep Learning, Computer Vision and Decision Tree\n",
    "### Guide: Dr. Sunil Kumar\n",
    "### Students: Ananya Agrawal (199303010) & Hardik Srivastava (199303069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training a CNN Model to create crowd heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Reshape, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import cv2\n",
    "import keras.backend as K\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"B\"\n",
    "train_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/train/'\n",
    "train_den_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/train_den/'\n",
    "val_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/val/'\n",
    "val_den_path = './data/formatted_trainval/shanghaitech_part_' + dataset + '_patches_9/val_den/'\n",
    "test_path = './data/original/shanghaitech/part_' + dataset + '_final/test_data/images/'\n",
    "test_den_path = './data/original/shanghaitech/part_' + dataset + '_final/test_data/ground_truth_csv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images = os.listdir(train_path)\n",
    "num_training_images = len(training_images)\n",
    "\n",
    "train_data = []\n",
    "for i in range(num_training_images):\n",
    "    if i % 100 == 0:\n",
    "        print(i, '/', num_training_images, \"Loaded\")\n",
    "    name = training_images[i]\n",
    "    img = cv2.imread(train_path + name, 0)\n",
    "    img = np.array(img)\n",
    "    img = (img - 127.5) / 128\n",
    "    den = np.loadtxt(open(train_den_path + name[:-4] + '.csv'), delimiter = \",\")\n",
    "    den_quarter = np.zeros((int(den.shape[0] / 4), int(den.shape[1] / 4)))\n",
    "    for i in range(len(den_quarter)):\n",
    "        for j in range(len(den_quarter[0])):\n",
    "            for p in range(4):\n",
    "                for q in range(4):\n",
    "                    den_quarter[i][j] += den[i * 4 + p][j * 4 + q]\n",
    "    train_data.append([img, den_quarter])\n",
    "print('Training Data Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = os.listdir(test_path)\n",
    "num_test_images = len(test_images)\n",
    "\n",
    "test_data = []\n",
    "for i in range(num_test_images):\n",
    "    if i % 50 == 0:\n",
    "        print(i, '/', num_test_images)\n",
    "    name = 'IMG_' + str(i + 1) + '.jpg'\n",
    "    img = cv2.imread(test_path + name, 0)\n",
    "    img = np.array(img)\n",
    "    img = (img - 127.5) / 128\n",
    "    den = np.loadtxt(open(test_den_path + name[:-4] + '.csv'), delimiter = \",\")\n",
    "    den_quarter = np.zeros((int(den.shape[0] / 4), int(den.shape[1] / 4)))\n",
    "    for i in range(len(den_quarter)):\n",
    "        for j in range(len(den_quarter[0])):\n",
    "            for p in range(4):\n",
    "                for q in range(4):\n",
    "                    den_quarter[i][j] += den[i * 4 + p][j * 4 + q]\n",
    "    test_data.append([img, den_quarter])\n",
    "        \n",
    "print('Test data Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling data around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate X, Y training and testing data lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for d in train_data:\n",
    "    X_train.append(np.reshape(d[0], (d[0].shape[0], d[0].shape[1], 1)))\n",
    "    y_train.append(np.reshape(d[1], (d[1].shape[0], d[1].shape[1], 1)))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for d in test_data:\n",
    "    X_test.append(np.reshape(d[0], (d[0].shape[0], d[0].shape[1], 1)))\n",
    "    y_test.append(np.reshape(d[1], (d[1].shape[0], d[1].shape[1], 1)))\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup helper methods to calculate Mean Absolute Error, Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(y_real, y_pred):\n",
    "    return abs(K.sum(y_real) - K.sum(y_pred))\n",
    "\n",
    "def calc_mse(y_real, y_pred):\n",
    "    return ((K.sum(y_real) - K.sum(y_pred)) * (K.sum(y_real) - K.sum(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (None, None, 1))\n",
    "conv_m = Conv2D(20, (7, 7), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_m = MaxPooling2D(pool_size = (2, 2))(conv_m)\n",
    "conv_m = (conv_m)\n",
    "conv_m = Conv2D(40, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "conv_m = MaxPooling2D(pool_size = (2, 2))(conv_m)\n",
    "conv_m = Conv2D(20, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "conv_m = Conv2D(10, (5, 5), padding = 'same', activation = 'relu')(conv_m)\n",
    "#conv_m = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_m)\n",
    "\n",
    "conv_s = Conv2D(24, (5, 5), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_s = MaxPooling2D(pool_size = (2, 2))(conv_s)\n",
    "conv_s = (conv_s)\n",
    "conv_s = Conv2D(48, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "conv_s = MaxPooling2D(pool_size = (2, 2))(conv_s)\n",
    "conv_s = Conv2D(24, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "conv_s = Conv2D(12, (3, 3), padding = 'same', activation = 'relu')(conv_s)\n",
    "#conv_s = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_s)\n",
    "\n",
    "conv_l = Conv2D(16, (9, 9), padding = 'same', activation = 'relu')(inputs)\n",
    "conv_l = MaxPooling2D(pool_size = (2, 2))(conv_l)\n",
    "conv_l = (conv_l)\n",
    "conv_l = Conv2D(32, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "conv_l = MaxPooling2D(pool_size = (2, 2))(conv_l)\n",
    "conv_l = Conv2D(16, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "conv_l = Conv2D(8, (7, 7), padding = 'same', activation = 'relu')(conv_l)\n",
    "#conv_l = Conv2D(1, (1, 1), padding = 'same', activation = 'relu')(conv_l)\n",
    "\n",
    "conv_merge = Concatenate(axis = 3)([conv_m, conv_s, conv_l])\n",
    "result = Conv2D(1, (1, 1), padding = 'same')(conv_merge)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = result)\n",
    "\n",
    "adam = Adam(lr = 1e-4)\n",
    "model.compile(loss = 'mse', optimizer = adam, metrics = [calc_mae, calc_mse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traing model (while occasionally saving model and weights to disk for checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mae = 10000\n",
    "best_mae_mse = 10000\n",
    "best_mse = 10000\n",
    "best_mse_mae = 10000\n",
    "\n",
    "for i in range(200):\n",
    "    model.fit(X_train, y_train, epochs = 3, batch_size = 1, validation_split = 0.2)\n",
    "\n",
    "    score = model.evaluate(X_test, y_test, batch_size = 1)\n",
    "    score[2] = math.sqrt(score[2])\n",
    "    print(score)\n",
    "    if score[1] < best_mae:\n",
    "        best_mae = score[1]\n",
    "        best_mae_mse = score[2]\n",
    "        json_string = model.to_json()\n",
    "        open('./model/model.json', 'w').write(json_string)\n",
    "        model.save_weights('./model/weights.h5')\n",
    "    if score[2] < best_mse:\n",
    "        best_mse = score[2]\n",
    "        best_mse_mae = score[1]\n",
    "\n",
    "    print('Best mae: ', best_mae, '(', best_mae_mse, ')')\n",
    "    print('Best mse: ', '(', best_mse_mae, ')', best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generating CSV dataset for training a Decision Tree Classifier for classifying images into Sparse, Medium, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting source data directory, along with classes and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = \"./data_subset\"\n",
    "classes = [\"dense\", \"medium\", \"sparse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mat4py import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining methods to handle clustering logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_CONSTANT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgnum, ab, ac, ad, ae, bc, bd, be, cd, ce, de, target\n",
    "IMG_NUM = []\n",
    "SOURCE_DATASET = []\n",
    "AB = []\n",
    "AC = []\n",
    "AD = []\n",
    "AE = []\n",
    "BC = []\n",
    "BD = []\n",
    "BE = []\n",
    "CD = []\n",
    "CE = []\n",
    "DE = []\n",
    "TARGET = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads mat file and returns formatted list of points in it\n",
    "def read_pts(file):\n",
    "    data = loadmat(file)\n",
    "    pts = []\n",
    "    for loc in data['image_info']['location']:\n",
    "        pts.append((int(loc[0]), int(loc[1])))\n",
    "    return pts\n",
    "\n",
    "# creates clusters of pts list\n",
    "def make_clusters(pts):\n",
    "    est = KMeans(K_CONSTANT)\n",
    "    est.fit(pts)\n",
    "    y_kmeans = est.predict(pts)\n",
    "    # cluster list\n",
    "    cluster_list = [[], [], [], [], []]\n",
    "    for index in range(len(pts)):\n",
    "        cluster_list[y_kmeans[index]].append(pts[index])\n",
    "    return cluster_list\n",
    "\n",
    "# returns dist between 2 points\n",
    "def pt_dist(p1, p2):\n",
    "    xx = p1[0] - p2[0]\n",
    "    yy = p1[1] - p2[1]\n",
    "    return math.sqrt(xx*xx + yy*yy)\n",
    "\n",
    "# returns min dist between cluster 1 and cluster 2\n",
    "def min_dist(c1, c2):\n",
    "    min = 9999999\n",
    "    for p1 in c1:\n",
    "        for p2 in c2:\n",
    "            d = pt_dist(p1, p2)\n",
    "            if min > d:\n",
    "                min = d\n",
    "    return min\n",
    "\n",
    "def handle_class(classname, source_dataset):\n",
    "    files = os.listdir(source_dir + \"/\" + classname)\n",
    "    for file in files:\n",
    "        if file.endswith(\".mat\"):\n",
    "            imgname = str(file)[7:-4]\n",
    "            IMG_NUM.append(imgname)\n",
    "            SOURCE_DATASET.append(source_dataset)\n",
    "            TARGET.append(classname)\n",
    "            all_clusters = make_clusters(read_pts(source_dir + \"/\" + classname + \"/\" + file))\n",
    "            AB.append(min_dist(all_clusters[0], all_clusters[1]))\n",
    "            AC.append(min_dist(all_clusters[0], all_clusters[2]))\n",
    "            AD.append(min_dist(all_clusters[0], all_clusters[3]))\n",
    "            AE.append(min_dist(all_clusters[0], all_clusters[4]))\n",
    "            BC.append(min_dist(all_clusters[1], all_clusters[2]))\n",
    "            BD.append(min_dist(all_clusters[1], all_clusters[3]))\n",
    "            BE.append(min_dist(all_clusters[1], all_clusters[4]))\n",
    "            CD.append(min_dist(all_clusters[2], all_clusters[3]))\n",
    "            CE.append(min_dist(all_clusters[2], all_clusters[4]))\n",
    "            DE.append(min_dist(all_clusters[3], all_clusters[4]))\n",
    "\n",
    "\n",
    "handle_class(classes[0], \"A\")\n",
    "handle_class(classes[1], \"B\")\n",
    "handle_class(classes[2], \"B\")\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "dataset[\"IMG_NUM\"] = IMG_NUM\n",
    "dataset[\"SOURCE_DATASET\"] = SOURCE_DATASET\n",
    "dataset[\"AB\"] = AB\n",
    "dataset[\"AC\"] = AC\n",
    "dataset[\"AD\"] = AD\n",
    "dataset[\"AE\"] = AE\n",
    "dataset[\"BC\"] = BC\n",
    "dataset[\"BD\"] = BD\n",
    "dataset[\"BE\"] = BE\n",
    "dataset[\"CD\"] = CD\n",
    "dataset[\"CE\"] = CE\n",
    "dataset[\"DE\"] = DE\n",
    "dataset[\"TARGET\"] = TARGET\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"./model/exported.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using the CSV dataset for training a Decision Tree Classifier for classifying images into Sparse, Medium, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = pd.read_csv(\"./model/exported.csv\", index_col=0)\n",
    "cluster_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing X and Y slices of data, where X is our source data and Y contains the Target Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cluster_data.values[:, 2:-1]\n",
    "Y = cluster_data.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up functions to create decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform training with giniIndex.\n",
    "def train_using_gini(X_train, X_test, y_train):\n",
    "    # Creating the classifier object\n",
    "    clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=None)\n",
    "    # Performing training\n",
    "    clf_gini.fit(X_train, y_train)\n",
    "    return clf_gini\n",
    "\n",
    "# Function to make predictions\n",
    "def prediction(X_test, clf_object):  \n",
    "    # Predicton on test with giniIndex\n",
    "    y_pred = clf_object.predict(X_test)\n",
    "    print(\"Predicted values:\")\n",
    "    print(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):      \n",
    "    print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred))\n",
    "    print (\"Accuracy : \", accuracy_score(y_test,y_pred) * 100)\n",
    "    print(\"Report : \", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = train_using_gini(X_train, X_test, y_train)\n",
    "\n",
    "y_pred_gini = prediction(X_test, clf_gini)\n",
    "cal_accuracy(y_test, y_pred_gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Using the trained model to generate heatmaps for a provided crowd image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "from pyheatmap.heatmap import HeatMap\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Model and Weights from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "model.load_weights(\"weights.h5\")\n",
    "\n",
    "print(\"Loaded model from disk successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up path of input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = \"/home/hardik/Projects/Minor-Project/new/IMG_233.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and preprocessing input image to correspond to input layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(cv2.imread(input_image, 0))\n",
    "img = (img - 127.5) / 128\n",
    "inputs = np.reshape(img, [1, 768, 1024, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferencing through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(inputs)\n",
    "predicted_count = np.sum(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing predicted count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted Count:\", predicted_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating density numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_den = np.reshape(outputs, (outputs.shape[1], outputs.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den_resized = np.zeros((y_p_den.shape[0] * 4, y_p_den.shape[1] * 4))\n",
    "for i in range(den_resized.shape[0]):\n",
    "    for j in range(den_resized.shape[1]):\n",
    "        den_resized[i][j] = y_p_den[int(i / 4)][int(j / 4)] / 16\n",
    "den = den_resized\n",
    "count = np.sum(den)\n",
    "den = den * 10 / np.max(den)\n",
    "\n",
    "crowd_img = cv2.imread(input_image, 1)\n",
    "\n",
    "data = []\n",
    "pts = []\n",
    "\n",
    "for j in range(len(den)):\n",
    "    for i in range(len(den[0])):\n",
    "        for k in range(int(den[j][i])):\n",
    "            data.append([i + 1, j + 1])\n",
    "            pts.append((i + 1, j + 1))\n",
    "\n",
    "hm = HeatMap(data, base=input_image)\n",
    "hm.heatmap(save_as = 'output.png')\n",
    "print(\"Heatmap Generated to output.png\")\n",
    "display(Image.open(\"output.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Finding group of cluster distances and running our inference through Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate combination of distances for all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters = make_clusters(pts)\n",
    "\n",
    "ab = min_dist(all_clusters[0], all_clusters[1])\n",
    "ac = min_dist(all_clusters[0], all_clusters[2])\n",
    "ad = min_dist(all_clusters[0], all_clusters[3])\n",
    "ae = min_dist(all_clusters[0], all_clusters[4])\n",
    "bc = min_dist(all_clusters[1], all_clusters[2])\n",
    "bd = min_dist(all_clusters[1], all_clusters[3])\n",
    "be = min_dist(all_clusters[1], all_clusters[4])\n",
    "cd = min_dist(all_clusters[2], all_clusters[3])\n",
    "ce = min_dist(all_clusters[2], all_clusters[4])\n",
    "de = min_dist(all_clusters[3], all_clusters[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create entry and get prediction for our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_entry = [ab, ac, ad, ae, bc, bd, be, cd, ce, ce]\n",
    "print(check_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gini = prediction([check_entry], clf_gini)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
